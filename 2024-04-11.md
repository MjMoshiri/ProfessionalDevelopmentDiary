# 2024-04-11

- **[MIT 6.034 Artificial Intelligence Course](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/)**: The focus was on the core principles and applications of probability in AI, emphasizing:
    - **Review on Joint Probabilities and Conditional Probabilities**: Refreshed and deepened understanding of joint probabilities and conditional probabilities, which are foundational to reasoning under uncertainty.
    - **Bayesian Networks**: Delved into Bayesian networks, a type of statistical model that uses a graphical method to represent a set of variables and their conditional dependencies via a directed acyclic graph (DAG). These networks are powerful tools for modeling complex systems where we need to consider the interdependencies between different factors.
    - **Naive Bayes and Real-life Applications**: Specifically focused on the Naive Bayes classifier, a simple yet effective probabilistic model based on applying Bayes' theorem with the "naive" assumption of conditional independence between every pair of features given the value of the class variable. Explored its significant applications in real life, such as disease diagnosis, highlighting how we can leverage these probabilistic models to make informed decisions based on uncertain information.

- **[3Blue1Brown - Transformer Networks Series](https://www.youtube.com/3blue1brown)**: Watched the first two episodes of this series, focusing on transformer networks and their significance in deep learning, particularly using GPT-3 as an illustrative example. Key takeaways include:
    - **[Episode 1 - Introduction to Transformer Networks](https://youtu.be/wjZofJX0v4M?si=_LMbHZKyj5ZiXOdH)**: This episode provided a foundational understanding of what transformer networks are, detailing their layers, size (with an explanation of parameters), word embeddings, and the general deep learning structure that underpins these networks.
    - **[Episode 2 - The Attention Mechanism](https://youtu.be/eMlx5fFNoYc?si=naJY6Cu2R2p9VG-U)**: Dived deeper into the attention layer, explaining its functionality, the interpretation of the concept, and the roles of queries, keys, values, and the output matrix in shaping how the network processes and prioritizes information.

**Tags**: MIT, Artificial Intelligence, Probability Interfaces, Bayesian Networks, Naive Bayes, Transformer Networks, 3Blue1Brown, Deep Learning
